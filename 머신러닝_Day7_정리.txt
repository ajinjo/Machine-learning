로지스틱 회귀
- 선형 회귀 방식을 분류에 적용한 알고리즘
- 선형 회귀 방식을 기반으로 하되
- 시그모이드 함수를 이용해 분류 수행

로지스틱 회귀가 선형 회귀와 다른 점
- 학습을 통해 선형 함수의 회귀선을 찾는 것이 아니라
- 시그모이드(Sigmoid) 함수의 최적선을 찾고
- 이 시그모이드 함수의 반환값을 확률로 간주해
- 확률에 따라 분류를 결정한다는 것


시그모이드(Sigmoid) 함수
- S자 커브 형태의 시그모이드 함수 이용
- 좀 더 정확하게 0과 1에 대해 분류 가능
- 로지스틱 회귀 방법
- 선형 회귀 방식을 기반으로 하되 시그모이드 함수를 이용해서 분류를 수행하는 회귀

로지스틱 회귀 정리
- 가볍고 빠르면서도
- 이진 분류 예측 성능도 뛰어남
- 따라서 이진 분류기의 기본 모델로 사용하는 경우가 많음
- 또한 희소 행렬로 표현되는 텍스트 기반의 분류에서도 자주 사용됨


회귀 실습 - 캐글 주택 가격 회귀 분석 : 고급 회귀 기법 사용

전체 회귀 분석 과정  
(1) 데이터 전처리  
(2) 선형 회귀 모델 학습/예측/평가  
(3) 회귀 트리 모델 학습/예측/평가  
(4) 회귀 모델의 예측 결과 혼합을 통한 최종 예측  
(5) 스태킹 앙상블 모델을 통한 회귀 예측


(1) 데이터 전처리  
Null
정규 분포인지 확인
로그 변환 및 환원
원-핫 인코딩 변환

(2) 선형 회귀 모델 학습/예측/평가
LinearRegression, Ridge, Lasso 를 이용해서 선형 계열의 회귀 모델 
여러 개의 회귀 모델들에 대한 회귀 계수값과 칼럼명을 시각화
5 폴드 교차 검증으로 모델별 RMSE와 평균 RMSE 출력
릿지와 라쏘 모델의 alpha 값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 재학습/예측/평가 수행
앞의 최적화 alpha 값으로 학습 데이터로 학습, 테스트 데이터로 예측 및 평가 수행

데이터 세트를 추가적으로 가공해서 모델 튜닝 좀 더 진행
-(1) 피처 데이터 세트의 데이터 분포도 확인 - 왜곡도 높은 피처들은 로그 변환 수행
-(2) 이상치 데이터 처리
5 폴드 교차 검증으로 모델별 RMSE와 평균 RMSE 출력
릿지와 라쏘 모델의 alpha 값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 재학습/예측/평가 수행
앞의 최적화 alpha 값으로 학습 데이터로 학습, 테스트 데이터로 예측 및 평가 수행



(3) 회귀 트리 모델 학습/예측/평가
XGBoost와 LightGBM 이용해서 학습/예측/평가 수행
XGBoost와 LightGBM 모두 수행 시간이 오래 걸릴 수 있는 관계로
하이퍼 파라미터 설정을 미리 적용한 상태로
5 폴트 세트에 대한 평균 RMSE 값 추출

XGBoost 회귀 트리 적용
LightGBM 회귀 트리 적용

트리 회귀 모델의 피처 중요도 시각화


(4) 회귀 모델의 예측 결과 혼합을 통한 최종 예측
앞에서 구한 릿지 모델(40%)과 라쏘 모델(60%) 혼합
최종 혼합 모델의 RMSE가 개별 모델보다 성능면에서 약간 개선됨


(5) 스태킹 앙상블 모델을 통한 회귀 예측

스태킹(Stacking)

개별적인 여러 알고리즘을 서로 결합해서 예측 결과를 도출한다는 점에서는
배깅 및 부스팅 방식과 동일하지만
큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로
다시 예측을 수행하는 것

메타 모델
- 개별 모델의 예측된 데이터 세트를 기반으로
- 다시 학습하고 예측하는 방식

# 개별 모델이 반환한 학습 및 테스트용 데이터 세트를 Stacking 형태로 결합.  
Stack_final_X_train = np.concatenate((ridge_train, lasso_train, 
                                      xgb_train, lgbm_train), axis=1)
Stack_final_X_test = np.concatenate((ridge_test, lasso_test, 
                                     xgb_test, lgbm_test), axis=1)


# 최종 메타 모델은 라쏘 모델을 적용. 
meta_model_lasso = Lasso(alpha=0.0005)

#기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정.
meta_model_lasso.fit(Stack_final_X_train, y_train)
final = meta_model_lasso.predict(Stack_final_X_test)
mse = mean_squared_error(y_test , final)
rmse = np.sqrt(mse)

# 결과
# 최종적으로 스태킹 회귀 모델을 적용할 결과
# 테스트 데이터 세트에서 RMSE가 0.098로
# 현재까지 가장 좋은 성능 평가를 보여줌

